\chapter{Introduction} % Chapter title

\label{chapter:introduction} % For referencing the chapter elsewhere, use \ref{chapter:computational_neuro} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}
%----------------------------------------------------------------------------------------

\section{Status Quo}
\subsection{Quantum Neural Network}
Classification of data is a canonical problem in machine learning and has made great strides towards getting classical computers to classify data \cite{Killoran_2019,ClassificationWithQNN}. A reliable method for classifying data is offered by artificial neural networks using supervised learning. Based on the training data, the artificial neural network learns to find a hypothesis that can make predictions, which are as accurate as possible. To determine the accuracy, the learned hypothesis can be used to evaluate testing data whose solution is known to the classification, but has not been previously used for training. The idea of a supervised learning algorithm can also be used to train a quantum based, neural network.\par
Currently, a common way to create, test and train artificial quantum neural networks is a variational quantum algorithm using a hybrid classical-quantum approach, where only the artificial neural network is implemented as a quantum circuit and the optimization of the parameters done classically\cite{Cerezo_2021,mccleanBarrenPlateausQuantum2018,Zhao_2021,schuld_SQMLmodelsAreKernelMethods,sim_expressibility_2019,Abbas_2021}. There is also an obvious connection between the variational quantum circuit and the support vector machine in terms of their measurement behaviour as a linear decision boundary in their feature space, indicating that supervised quantum models behave like kernel methods. Furthermore, quantum models map data into a high-dimensional feature space when encoding them into the quantum state using a feature map, as support vector machines do\cite{havlicekSupervisedLearningQuantum2019,ThomsenComparingQNNs_QSVM}. Schuld et al.\cite{schuld_SQMLmodelsAreKernelMethods} state that “approaching quantum machine learning from a kernel perspective can have profound implications for the way we think about it” and that “most quantum models can be formulated as general support vector machines with a kernel evaluated on a quantum computer” giving a clear hint that machine learning quantum models at least partially implement kernel like behaviour. In addition, previously published work have shown that proposed quantum enhanced support vector machines, short QSVM, can outshine classical SVM algorithms, which could potentially mean that the same holds true for a quantum neural network\cite{havensteinComparisonsPerformanceQuantum2019,yangSupportVectorMachines2019}.\par
Nevertheless, in contrast to quantum support vector machines there is currently no general implementation template for quantum neural networks, but various implementation proposals with different classification applications and goals. A problematic factor is that current hardware offerings are error-prone and have limited access. Any larger experiments with larger datasets cannot be evaluated without a major effort. Nevertheless, access to quantum computers has become steadily easier and currently undergoing a transformation from purely academic to an industrially used technology\cite{schuldCircuitcentricQuantumClassifiers2020}. Moreover, it is widely believed that by exploiting the potential quantum effects such as superposition, entanglement, and interference, one can achieve potential benefits such as acceleration of training, faster processing, parallelization, and reduced complexity. Whether quantum-based neural networks are more powerful than quantum support vector machines or their classical counterparts remains to be seen.

\subsection{Multiple Query Optimization}
To explain what exactly multiple query optimization is, the simple task of buying groceries in a store can be used as an example. An entity $\mathcal{U}$ has a list $\mathcal{L}$, and each $x \in \mathcal{L}$ is an object he has to retrieve from the store. In this case, $\mathcal{L}$ contains four elements $\mathcal{L} = \left\{ \text{Apple}, \text{Pineapple}, \text{Beef}, \text{Wine}\right\}$. Each element $x$ has an isle in the store where it is located. To retrieve the item, $\mathcal{U}$ has to go to said isle and pick it up. By looking at the list, we can see that the elements “Pineapple” and “Apple” are both fruits, therefore situated at the same isle. To optimize the path through the store, $\mathcal{U}$ can combine the operation of retrieving the elements “Pineapple” and “Apple” so that he only needs to go to the fruit isle once, instead of having to circle the store multiple times. In its essence, $\mathcal{U}$ did the optimization of a combinational problem, in this case finding the optimal path, by himself. This task by itself isn't unknown to humans – we do this multiple times throughout any given day. \par
The store is now enhanced with an autonomous helper that will retrieve any item the customers want. Instead of each entity $\mathcal{U}_i$ having to retrieve the items themselves, the helper does it for them. When multiple customers $\mathcal{U}_i$ arrive and supply their list $\mathcal{L}_i$ to the helper, then there are two possible ways of working through them. The first and most basic one, is to go step by step through each list and retrieve the items. In this case, the helper can rearrange the list to shorten the total path – but he still goes through each list in an isolated fashion. This is usually referred to as \emph{query optimization}. The second option would be to look at all the lists and combine single retrievals together, which would be the \emph{multiple query optimization}. As an example, the helper could retrieve all the fruit required by all entities $\mathcal{U}_i$ at the same time. After finding the best combinations, he can then select the shortest route to execute these retrievals. This optimization by itself can save the required waiting time for the entities, but also the total walking distance for the helper.\par
When it comes to the real multiple query optimization, it involves a database $\mathcal{D}$ that replaces our store and our helper, where multiple entities $\mathcal{U}_i$ want to retrieve information $\mathcal{I}_i \in \mathcal{D}$. For this, the request of information, referred to as \emph{query}, is decomposed into single operations. Where the grocery store example had different isles and the path to reach them, $\mathcal{D}$ has the data stored in tables it has to load into memory and, depending on the level of data normalization in $\mathcal{D}$\cite{semantic_relational_data_model}, different tables might have to be combined and filtered to result in the desired information $\mathcal{I}_i$. Considering the number of users a modern application serves\cite{uber_technologies_inc_uber_2022}, one can imagine the there are always multiple queries being executed at the same time and could potentially be combined.\par
The problem of finding the optimal combination is of quadratic nature; which means that in its most primitive form the runtime is $O(n^2)$ to find the best combination. The problem itself is also an NP-Hard\cite{sellis_multiple-query_1990}. This has been an ongoing problem to solve, with classical algorithms\cite{kathuria_provable_mqo} available that can find a \emph{good enough} solution\footnote{A good enough solution is one that will find a combination that still saves time, but might not be the best available solution} in a usable timeframe. There are also quantum enhanced proposals\cite{fankhauser_multiple_2021}, which will be used for direct comparison in this thesis. As it stands, there has yet to be one \emph{final} solution to completely solve the problem in a way that can be used in running systems without compromising the user experience.

\section{Goals}
\subsection{Quantum Neural Network}
To allow an adequate comparison, different variational quantum circuits are created which resemble quantum based neural networks. Using a collection of datasets which equals the one chosen by Smailov et al.\cite{smailovQuantumMachineLearning2021}, the created circuits are trained and evaluated. The achieved results from the datasets are used to directly compare to the results achieved in the quantum support vector machine from Smailov et al.\cite{smailovQuantumMachineLearning2021}. The optimization itself is executed on a quantum simulator, whereas the evaluation itself is done on quantum hardware. \par
The final goal is to use the achieved results and conclude how well a variational quantum circuit and its design behaves in comparison to a quantum support vector machine.\par
Another goal of this paper is to evaluate different variational quantum circuits which closely resemble quantum neural networks for classification and further compare the results with the quantum support vector machine approach done by Smailov et al.\cite{smailovQuantumMachineLearning2021}. The optimizable parameters, so-called weights, are pre-trained by a variational quantum algorithm with a hybrid classical-quantum approach on quantum simulators. For this, five different binary datasets of two different sizes and additionally the \textit{iris} dataset with all data points and all three classes. Finally, the quantum circuits are evaluated and analysed on freely available real quantum computers from IBM using the datasets and the pre-trained weights. To finish, the differences between a quantum neural network and quantum support vector machine are analysed.


\subsection{Multiple Query Optimization}
Initially, the task is to analyse the problem space of a multiple query optimization problem and to design a circuit that can solve it. To evaluate the circuit itself, a data generator is to be created that can generate problems to solve. 
Using the created circuit as well as the problem generator, the circuit is to be optimized using training algorithms from classical machine learning. Due to hardware limitations, optimization is done exclusively on a quantum simulator, whilst evaluation is done on the simulator and on real hardware. This allows to showcase the difference in achievable results when using simulator and real hardware.\par
Finally, the circuit itself is evaluated and compared to an already proposed quantum solution\cite{fankhauser_multiple_2021}. Due to the circumstances involving current accessibility of quantum hardware and noise affecting any circuits run on it, factors like total runtime, training etc. on real hardware are not evaluated.

\clearpage
