\chapter{Introduction} % Chapter title

\label{chapter:introduction} % For referencing the chapter elsewhere, use \ref{chapter:computational_neuro} 

%----------------------------------------------------------------------------------------

% Define some commands to keep the formatting separated from the content 
\newcommand{\keyword}[1]{\textbf{#1}}
\newcommand{\tabhead}[1]{\textbf{#1}}
\newcommand{\code}[1]{\texttt{#1}}
\newcommand{\file}[1]{\texttt{\bfseries#1}}
\newcommand{\option}[1]{\texttt{\itshape#1}}
%----------------------------------------------------------------------------------------

\section{Status Quo}
\todo{→Literaturrecherche
Stand der Technik: Bisherige Lösungen des Problems und deren Grenzen}

\subsection{Multiple Query Optimization}
\todo{Here should be a concrete example of query optimization and afterwards extended by multi-query optimization.}
To explain what exactly multiple query optimization is, the simple task of buying groceries in a store can be used as an example. An entity $\mathcal{U}$ has a list $\mathcal{L}$, and each $x \in \mathcal{L}$ is an object he has to retrieve from the store. In this case, $\mathcal{L}$ contains four elements $\mathcal{L} = \left\{ \text{Apple}, \text{Pineapple}, \text{Beef}, \text{Wine}\right\}$. Each element $x$ has an isle in the store where it is located. To retrieve the item, $\mathcal{U}$ has to go to said isle and pick it up. By looking at the list, we can see that the elements “Pineapple” and “Apple” are both fruits, therefore situated at the same isle. To optimize the path through the store, $\mathcal{U}$ can combine the operation of retrieving the elements “Pineapple” and “Apple” so that he only needs to go to the fruit isle once, instead of having to circle the store multiple times. In its essence, $\mathcal{U}$ did the optimization of a combinational problem, in this case finding the optimal path, by himself. This task by itself isn't unknown to humans – we do this multiple times throughout any given day. \par
The store is now enhanced with an autonomous helper that will retrieve any item the customers want. Instead of each entity $\mathcal{U_i}$ having to retrieve the items themselves, the helper does it for them. When multiple customers $\mathcal{U}_i$ arrive and supply their list $\mathcal{L}_i$ to the helper, then there are two possible ways of working through them. The first and most basic one, is to go step by step through each list and retrieve the items. In this case, the helper can rearrange the list to shorten the total path – but he still goes through each list in an isolated fashion. The second option would be to look at all the lists together and combine single retrievals together. As an example, the helper could retrieve all the fruit required by all entities $\mathcal{U}_i$ at the same time. After finding the best combinations, he can then select the shortest route to execute these retrievals. This optimization by itself can save the required waiting time for the entities, but also the total walking distance for the helper.\par
When it comes to the essence of multiple query optimization, it involves a database $\mathcal{D}$ that replaces our store and our helper, where multiple entities $\mathcal{U}_i$ want to retrieve information $\mathcal{I}_i \in \mathcal{D}$. For this, the request of information, referred to as \emph{query}, is decomposed into single operations. Where the grocery store example had different isles and the path to reach them, $\mathcal{D}$ has the data stored in tables it has to load into memory and depending on the level of data normalization in $\mathcal{D}$\cite{semantic_relational_data_model}, different tables might have to be combined and filtered to result in the desired information $\mathcal{I}_i$. Considering the number of users a modern application serves\cite{uber_technologies_inc_uber_2022}, one can imagine the there are always multiple queries being executed at the same time and could potentially be combined.\par
The problem of finding the optimal combination is of quadratic nature; which means that in its most primitive form the runtime is $O(n^2)$ to find the best combination. The problem itself is also an NP-Hard\cite{sellis_multiple-query_1990}. This has been an ongoing problem to solve, with classical algorithms\cite{kathuria_provable_mqo} available that can find a \emph{good enough} solution\footnote{A good enough solution is one that will find a combination that still saves time, but might not be the best available solution} in a usable timeframe. There are also quantum enhanced proposals\cite{fankhauser_multiple_2021}, which will be used for direct comparison in this thesis. As it stands, there has yet to be one \emph{final} solution to completely solve the problem in a way that can be used in running systems without compromising the user experience.

\subsection{Quantum Neural Network}
Classification of data is a canonical problem in machine learning and has made great strides towards getting classical computers to classify data \cite{Killoran_2019,ClassificationWithQNN}. A reliable method for classifying data is offered by artificial neural networks using supervised learning. Based on the training data, the artificial neural network learns to find a hypothesis that can make predictions, which are as accurate as possible. To determine the accuracy, the learned hypothesis can be used to evaluate testing data whose solution is known to the classification, but has not been previously used for training. The idea of a supervised learning algorithm can also be used to train a quantum based, neural network. Currently, a common way to create, test and train artificial quantum neural networks is a variational quantum algorithm using a hybrid classical-quantum approach, where only the artificial neural network is implemented as a quantum circuit and the optimization of the parameters done classically\cite{Cerezo_2021,mccleanBarrenPlateausQuantum2018,Zhao_2021,schuld_SQMLmodelsAreKernelMethods,sim_expressibility_2019,Abbas_2021}. \par
There is currently no general implementation template for quantum neural networks, but various implementation proposals with different classification applications and goals. A problematic factor of the current timeframe is that the available hardware is error-prone and its access limited. Any larger experiments with larger datasets cannot be evaluated without a major effort. Nevertheless, in recent years, access to quantum computers has become steadily easier and currently undergoing a transformation from purely academic to an industrially used technology\cite{schuldCircuitcentricQuantumClassifiers2020}. Moreover, it is widely believed that by exploiting the potential quantum effects such as superposition, entanglement, and interference, one can achieve potential benefits such as acceleration of training, faster processing, parallelization, and reduced complexity. \todo{Aus dem nichts wird QSVM erwähnt, braucht eine Anleitung. z.B. "Previously published work have shown that proposed quantum enhanced support vector machines, short QSVM, greatly outshine classical SVM algorithms, which could potentially mean that the same holds true for a quantum neural network."}Nevertheless, the question remains whether quantum-based neural networks are more powerful than quantum support vector machines or their classical counterparts.

\section{Goals}

\subsection{Multiple Query Optimization}
Initially, the task is to analyse the problem space of a multiple query optimization problem and to design a circuit that can solve it. To evaluate the circuit itself, a data generator is to be created that can generate problems to solve. 
Using the created circuit as well as the problem generator, the circuit is to be optimized using training algorithms from classical machine learning. Due to hardware limitations, optimization is done exclusively on a quantum simulator, whilst evaluation is done on the simulator and on real hardware. This allows to showcase the difference in achievable results when using simulator and real hardware.\par
Finally, the circuit itself is evaluated and compared to an already proposed quantum solution\cite{fankhauser_multiple_2021}. Due to the circumstances involving current accessibility of quantum hardware and noise affecting any circuits run on it, factors like total runtime, training etc. on real hardware are not evaluated.

\subsection{Quantum Neural Network}
To allow an adequate comparison, different variational quantum circuits are created which resemble quantum based neural networks. Using a collection of datasets which equals the one chosen by Smailov et al.\cite{smailovQuantumMachineLearning2021}, the created circuits are trained and evaluated. The achieved results from the datasets are used to directly compare to the results achieved the quantum support vector machine from Smailov et al.\cite{smailovQuantumMachineLearning2021}. The optimization itself is executed on a quantum simulator, whereas the evaluation itself is done on quantum hardware. \par
The final goal is to use the achieved results and conclude how well a variational quantum circuit and its design behaves in comparison to a quantum support vector machine.

Another goal of this paper is to evaluate different variational quantum circuits which closely resemble Quantum Neural Networks for classification and further compare the results with the Quantum Support Vector Machine approach done by Smailov et al.\cite{smailovQuantumMachineLearning2021}. We pre-trained the weights by a variational quantum algorithm with a hybrid classical-quantum approach on quantum simulators using five different binary datasets of two different sizes and additionally the \textit{iris} dataset with all datapoints and all of its three classes. Finally all quantum circuits are evaluated and analysed on freely available real quantum computers from IBM using the datasets and the pre-trained weights. We finally analyze the differences between the QNN and QSVM approach in more detail.

\clearpage
